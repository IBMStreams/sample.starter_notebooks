{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IBM Streams Event Store sample application\n",
    "\n",
    "This sample demonstrates creating a Streams Python application that ingests data into a Db2 Event Store table and viewing the metrics of the insert operation.\n",
    "\n",
    "In this notebook, you'll see examples of how to :\n",
    " 1. [Setup your data connections](#setup)\n",
    " 2. [Create the application](#create)\n",
    " 3. [Submit the application](#launch)\n",
    " 4. [Connect to the running instance to view metrics](#view)\n",
    " 5. [Stop the application](#cancel)\n",
    "\n",
    "# Overview\n",
    "\n",
    "**About the sample**\n",
    "\n",
    "This application simulates data tuples that are inserted as rows in a Db2 Event Store table.\n",
    "\n",
    "**How it works**\n",
    "\n",
    "The Python application created in this notebook is submitted to the IBM Streams service for execution. Once the application is running in the service, you can connect to the service instance from the notebook to retrieve the metrics.\n",
    "\n",
    "<img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/04/how-it-works.jpg\" alt=\"How it works\">\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n",
    "- [Db2 Event Store install and set up](https://www.ibm.com/support/knowledgecenter/en/SSGNPV_2.0.0/local/installsetup.html)\n",
    "\n",
    "\n",
    "<a name=\"setup\"></a>\n",
    "# 1. Setup\n",
    "### 1.1 Add credentials for the IBM Streams service\n",
    "\n",
    "In order to submit a Streams application you need to provide the name of the Streams instance.\n",
    "\n",
    "1. From the navigation menu, click **My instances**.\n",
    "2. Click the **Provisioned Instances** tab.\n",
    "3. Update the value of `streams_instance_name` in the cell below according to your Streams instance name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icpd_core import icpd_util\n",
    "streams_instance_name = \"my-instance\" ## Change this to Streams instance\n",
    "cfg=icpd_util.get_service_instance_details(name=streams_instance_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Optional: Upgrade the `streamsx.eventstore` Python package\n",
    "\n",
    "Uncomment and run the cell below if you want to upgrade to the latest version of the streamsx.eventstore package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install --user --upgrade streamsx.eventstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamsx.eventstore as es\n",
    "import streamsx.topology.context\n",
    "print(\"INFO: streamsx package version: \" + streamsx.topology.context.__version__)\n",
    "print(\"INFO: streamsx.eventstore package version: \" + es.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 (OPTIONAL) Update streamsx.eventstore toolkit with latest release\n",
    "Get the **latest** streamsx.eventstore toolkit release from GitHub.\n",
    "\n",
    "The Streams toolkit will be downloaded and added to the Topology.\n",
    "\n",
    "In this case the latest released IBM Streams EventStore **toolkit** is used for building the Streams application und not the toolkit that it located on the IBM Streams build service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamsx.eventstore as es\n",
    "\n",
    "# If url is None, then the latest toolkit release will be downloaded.\n",
    "url=None\n",
    "\n",
    "# download event store toolkit from GitHub\n",
    "eventstore_toolkit = es.download_toolkit(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Configure the connection to Db2 Event Store\n",
    "\n",
    "Update the name for the EventStore service/instance name.\n",
    "Run the cell below to configure the connection for the IBM Streams application.\n",
    "\n",
    "The connection details and credentials are stored in an application configuration for IBM Streams application. As name for application configuration is the name of Event Store instance used (`eventstore_instance`). The application configuration contains the following information:\n",
    "* Name of the database (`es_db`)\n",
    "* SCALA connection string (`es_connection`)\n",
    "* Event Store user and password (`es_user` and `es_password`)\n",
    "* Passwords for SSL certifcates (`es_truststore_password` and `es_keystore_password`)\n",
    "\n",
    "The \"EventStoreWriter\" in the application is configured with the name of the application configuration for the connection details stored in the `app_cfg` variable.\n",
    "Furthermore the \"EventStoreWriter\" requires the location of the truststore and keystore for the SSL connection (`es_truststore` and `es_keystore`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name according to your Event Store instance\n",
    "eventstore_instance='EventStore-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.rest import Instance\n",
    "import streamsx.topology.context\n",
    "import streamsx.eventstore as es\n",
    "\n",
    "cfg[streamsx.topology.context.ConfigParams.SSL_VERIFY] = False\n",
    "instance = Instance.of_service(cfg)\n",
    "\n",
    "# check if application configuration exists\n",
    "eventstore_app_config = instance.get_application_configurations(name=eventstore_instance)\n",
    "if eventstore_app_config:\n",
    "    # retrieve Event Store service details\n",
    "    eventstore_cfg=icpd_util.get_service_instance_details(name=eventstore_instance)\n",
    "    # get location of clientkeystore file and set values for es_truststore, es_keystore, app_cfg for later use\n",
    "    es_truststore, es_keystore = es.get_certificate(eventstore_cfg, name=eventstore_instance)\n",
    "    app_cfg = eventstore_instance\n",
    "else:\n",
    "    # retrieve Event Store service details\n",
    "    eventstore_cfg=icpd_util.get_service_instance_details(name=eventstore_instance)\n",
    "    es_db, es_connection, es_user, es_password, es_truststore, es_truststore_password, es_keystore, es_keystore_password = es.get_service_details(eventstore_cfg, name=eventstore_instance)\n",
    "    # create application configuration\n",
    "    app_cfg = es.configure_connection(instance, name=eventstore_instance, database=es_db, connection=es_connection, user=es_user, password=es_password, keystore_password=es_keystore_password, truststore_password=es_truststore_password)\n",
    "\n",
    "print(app_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create\"></a>\n",
    "# 2. Create the application\n",
    "This application is going to ingest simulated tuples.\n",
    "\n",
    "These simulated tuples are inserted as rows into a table using Db2 Event Store Scala API. This functionality is provided by `streamsx.eventstore.insert()` as \"EventStoreWriter\".\n",
    "\n",
    "In this example we create a table of two columns:\n",
    "**| ID:Long  |  NAME:String |**\n",
    "\n",
    "In the application we define the type `tuple<int64 ID, rstring NAME>` as IBM Streams schema.\n",
    "\n",
    "Important: The tuple field types and positions in the IBM Streams schema must match the field names in your IBM Db2 Event Store table schema exactly.\n",
    "\n",
    "Supported types: [Mapping of Event Store types to SPL types](https://ibmstreams.github.io/streamsx.eventstore/doc/spldoc/html/tk$com.ibm.streamsx.eventstore/op$com.ibm.streamsx.eventstore$EventStoreSink$1.html)\n",
    "\n",
    "\n",
    "In this example, the sharding key and the primary key are defined on the same column `ID`.\n",
    "\n",
    "Databases in IBM Db2 Event Store are partitioned into shards. Any given IBM Db2 Event Store node (in a multi-node IBM Db2 Event Store cluster) contains 0, 1, or N shards of the defined database. In addition to the mandatory shard key, you can optionally provide a primary key. When you define a primary key, IBM Db2 Event Store ensures that only a single version of each primary key exists in the database.\n",
    "\n",
    "**If the table does not exist in the database, then the table is created by the \"EventStoreWriter\".**\n",
    "\n",
    "All Streams applications start with  a `Topology` object, so start by creating one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.topology.topology import Topology\n",
    "\n",
    "topo = Topology(name=\"EventStoreInsertSample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create\"></a>\n",
    "### 2.0.1 (OPTIONAL) Add downloaded streamsx.eventstore toolkit to the topology\n",
    "\n",
    "In this case the toolkit downloaded in step 1.3.1 used for building the Streams application und not the toolkit that it located on the IBM Streams build service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eventstore_toolkit is not None:\n",
    "    # add event store toolkit to topology\n",
    "    streamsx.spl.toolkit.add_toolkit(topo, eventstore_toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define the application\n",
    "\n",
    "\n",
    "First step is to define a data source that produces the data being processed.\n",
    "For this, create a `Stream` called  `pulse` that will contain the simulated data with two attributes.\n",
    "A `Stream` is a potentially infinite sequence of tuples containing the data to be analyzed.\n",
    "\n",
    "Next, use the data source `Stream` as input for the \"EventStoreWriter\".\n",
    "\n",
    "**Define the table schema**\n",
    "\n",
    "* The table name is defined in the `table` variable. This is applied as `table` parameter to the `streamsx.eventstore.insert()` function\n",
    "* The table schema is specified by the Stream type `tuple<int64 ID, rstring NAME>`\n",
    "* Primary key of the table is applied as `primary_key` parameter to the `streamsx.eventstore.insert()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamsx.spl.op as op\n",
    "import streamsx.eventstore as es\n",
    "from streamsx.topology.schema import StreamSchema\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# This application creates a table with the name below\n",
    "table = 'StreamsEventStoreSampleTable'\n",
    "\n",
    "def generate_data():\n",
    "    counter = 0\n",
    "    while True:\n",
    "        #yield a random id and name\n",
    "        yield  {\"NAME\": \"id_\" + str(random.randint(0,10)), \"ID\": counter}\n",
    "        counter = counter + 1\n",
    "        time.sleep(0.01)\n",
    "\n",
    "# convert it to SPL schema for the Event stream operator\n",
    "tuple_schema = StreamSchema(\"tuple<int64 ID, rstring NAME>\")\n",
    "# Generates data for a stream of two attributes. Each attribute maps to a column using the same name of the DB2 Event Store table.\n",
    "pulse = topo.source(generate_data, name=\"GeneratedData\").map(lambda tpl: (tpl[\"ID\"], tpl[\"NAME\"]), schema=tuple_schema)\n",
    "\n",
    "\n",
    "# configure the number of tuples that are inserted as batch\n",
    "batch_size=100\n",
    "\n",
    "# insert tuple data into table as rows\n",
    "sink=es.insert(pulse, config=app_cfg, table=table, batch_size=batch_size, primary_key='ID', partitioning_key='ID', truststore=es_truststore, keystore=es_keystore, name='ES_Inserter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"launch\"></a>\n",
    "\n",
    "# 3. Submit the application\n",
    "A running Streams application is called a *job*. This next cell submits the application for execution and prints the resulting job id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.topology import context\n",
    "\n",
    "# Disable SSL certificate verification if necessary\n",
    "cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "# submit the topology 'topo'\n",
    "submission_result = context.submit (\"DISTRIBUTED\", topo, config = cfg)\n",
    "\n",
    "# The submission_result object contains information about the running application, or job\n",
    "if submission_result.job:\n",
    "    streams_job = submission_result.job\n",
    "    print (\"JobId: \", streams_job.id , \"\\nJob name: \", streams_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"view\"></a>\n",
    "\n",
    "# 4. Collect metrics from the job\n",
    "Now that the job is started, connect to the instance and view the metrics of the \"Event Store Writer\".\n",
    "\n",
    "The following metrics are collected: \n",
    "* nWriteSuccesses - number of successful inserts\n",
    "* nWriteFailures - number of failed inserts\n",
    "* insertTimeMin - minimum time of insert operation in ms\n",
    "* insertTimeAvg - average time of insert operation in ms\n",
    "* insertTimeMax - maximum time of insert operation in ms\n",
    "* nActiveInserts - indicates if inserts is active or not (1: active, 0: inactive)\n",
    "\n",
    "It prints one line per metrics fetch every 5 seconds for 2 minutes.\n",
    "\n",
    "**Collecting operator metrics**\n",
    "\n",
    "Each kind of operator provides different metrics with specific names. Find below example code to retrieve all operator metrics from all jobs in a Streams instance:\n",
    "\n",
    "---------\n",
    "```\n",
    "from streamsx.rest import Instance\n",
    "from streamsx.topology import context\n",
    "\n",
    "# Disable SSL certificate verification if necessary\n",
    "cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "# Streams instance object\n",
    "instance = Instance.of_service(cfg)\n",
    "# Get list of all running jobs in the Streams instance\n",
    "job_list = instance.get_jobs()\n",
    "for j in job_list:\n",
    "    job_id = j.id\n",
    "    job_name = j.name\n",
    "    print(\"JobId: \"+job_id + \" Name: \"+ job_name)\n",
    "    # Loop over each operator of the job\n",
    "    op_list = j.get_operators()\n",
    "    for op in op_list:\n",
    "        print(\"Operator name:\" + op.name + \" kind: \" + op.operatorKind)\n",
    "        # List all metrics of the operator\n",
    "        m = op.get_metrics()\n",
    "        if len(m) > 0:\n",
    "            print(\"Metric \"+m[0].name + \": \"+str(m[0].value))\n",
    "```\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.rest import Instance\n",
    "import streamsx.topology.context\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "# retrieve metrics from submitted job\n",
    "j = submission_result.job\n",
    "\n",
    "interval = 5 # collect metrics after n seconds\n",
    "n = 24 # collect n times, -1 infinite\n",
    "i = 0\n",
    "metrics_info = \"\"\n",
    "sleep(20) # initial delay since job startup takes some time\n",
    "while True:\n",
    "    i = i + 1\n",
    "    op = j.get_operators('ES_Inserter')[0]\n",
    "    if op.operatorKind == 'com.ibm.streamsx.eventstore::EventStoreSink':\n",
    "        time_info = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        for metric in (\"nWriteSuccesses\",\"nWriteFailures\",\"insertTimeMin\",\"insertTimeAvg\",\"insertTimeMax\",\"nActiveInserts\"):\n",
    "            m = op.get_metrics(name=metric)\n",
    "            if len(m) > 0:\n",
    "                metrics_info = metrics_info + \" \" + m[0].name + \": \" + str(m[0].value)\n",
    "\n",
    "        print(time_info + \" \" + metrics_info)\n",
    "    if i == n:\n",
    "        break\n",
    "    sleep(interval)\n",
    "    metrics_info = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 See job status \n",
    "\n",
    "You can view job status and logs by going to **My Instances** > **Jobs**. Find your job based on the id printed above.\n",
    "Retrieve job logs using the \"Download logs\" action from the job's context menu.\n",
    "\n",
    "To view other information about the job such as detailed metrics, access the Streams Console.  Go to **My Instances** > **Provisioned Instances**. Select the Streams instance and open the URL listed under *externalConsoleEndpoint* or *serviceConsoleEndpoint*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cancel\"></a>\n",
    "\n",
    "# 5. Cancel the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates a widget you can use to cancel the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel the job in the IBM Streams service\n",
    "submission_result.cancel_job_button()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also interact with the job through the [Job](https://streamsxtopology.readthedocs.io/en/stable/streamsx.rest_primitives.html#streamsx.rest_primitives.Job) object returned from `submission_result.job`\n",
    "\n",
    "For example, use `job.cancel()` to cancel the running job directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We started with a `Stream` called `pulse`, which contained the data we wanted to insert. Next, we used the `pulse` stream as input for the \"Event Store Writer\" to insert rows in the specified table.  \n",
    "\n",
    "After submitting the application to the Streams service, we connected to the instance via REST and displayed the \"Event Store Writer\" metrics to see the progress within the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
