{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Streams HDFS sample application\n",
    "This sample demonstrates creating a Streams Python application to connect to HDFS, perform some file operations on a HDFS, and viewing the results.\n",
    "\n",
    "In this notebook, you'll see examples of how to:\n",
    "- [Setup](#setup)\n",
    "- [Create HDFS credentials](#credentials)\n",
    "- [Create the application](#create)\n",
    "- [Submit the application](#submit)\n",
    "- [Connect to the running application to view data](#view)\n",
    "\n",
    "# Overview\n",
    "**About the sample**\n",
    "\n",
    "This application simulates data tuples that are inserted into a HDFS file (`write`) and reads all lines from a HDFS file. (`read`) <br/>\n",
    "The function `scan` scans a Hadoop Distributed File System directory for new or modified files and returns the file names.\n",
    "\n",
    "**How it works**\n",
    "   \n",
    "The Python application created in this notebook is submitted to the IBM Streams service for execution. Once the application is running in the service, you can connect to it from the notebook to retrieve the results.\n",
    "\n",
    "<img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/04/how-it-works.jpg\" alt=\"How it works\">\n",
    "\n",
    "\n",
    "### Documentation\n",
    "- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n",
    "\n",
    "\n",
    "\n",
    "## <a name=\"setup\"> </a> 1. Setup\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Add credentials for the IBM Streams service\n",
    "\n",
    "In order to submit a Streams application you need to provide the name of the Streams instance.\n",
    "\n",
    "1. From the navigation menu, click **My instances**.\n",
    "2. Click the **Provisioned Instances** tab.\n",
    "3. Update the value of `streams_instance_name` in the cell below according to your Streams instance name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icpd_core import icpd_util\n",
    "streams_instance_name = \"streams\" ## Change this to Streams instance\n",
    "\n",
    "try:\n",
    "    cfg=icpd_util.get_service_instance_details(name=streams_instance_name, instance_type=\"streams\")\n",
    "except TypeError:\n",
    "    cfg=icpd_util.get_service_instance_details(name=streams_instance_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import  the `streamsx.hdfs` package and verify the package version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamsx\n",
    "#!pip install streamsx.hdfs\n",
    "\n",
    "import streamsx.hdfs as hdfs\n",
    "import streamsx.topology.context\n",
    "print(\"INFO: streamsx package version: \" + streamsx.topology.context.__version__)\n",
    "print(\"INFO: streamsx.hdfs package version: \" + hdfs.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"credentials\"> </a> 1.3 Configure the connection to HDFS\n",
    "\n",
    "We need a HDFS credentials as JSON string to connect to a HDFS file system. This JSON string contains the HDFS credentials `user`, `password` and `webhdfs`.\n",
    "\n",
    "To create a HDFS credentials, please perform the following steps:\n",
    "\n",
    "- Create an IBM Analytic Engine (1.2) service on IBM cloud. <br/>\n",
    "  You need to have an IBM account to create an IAE service. <br/>\n",
    "  https://console.bluemix.net/catalog/?search=Analytics Engine\n",
    "\n",
    "- Copy the webhdfs hostname and the port and put it as `webhdfs` property into JSON string.\n",
    "- Reset the password as described in: <br/>\n",
    "     https://cloud.ibm.com/docs/services/AnalyticsEngine?topic=AnalyticsEngine-retrieve-cluster-credentials  <br>/\n",
    "     And copy the created password und put it as value of password property into JSON string.\n",
    "- Get the user name from IAE service credential und put it as value of `user` property into JSON string.\n",
    "  \n",
    "- Create a JSON string with the following attributes:\n",
    "```\n",
    "{\n",
    "  \"user\": \"your-hdfs-user-name\",\n",
    "  \"password\": \"your-hdfs-password\",\n",
    "  \"webhdfs\": \"webhdfs://your-hdfs-hostname:8443\"\n",
    "}\n",
    "```\n",
    "- The JSON string begins with `{` and ends with `}`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 options to put the HDFS credentials:\n",
    "\n",
    "1- Copy the credentials JSON string as described above in your clipboard and paste the credentials into `HDFS credentials:` prompt below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "hdfs_credentials=getpass.getpass('HDFS credentials:')\n",
    "print (hdfs_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Perform the steps [Connecting to data source](https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_current/cpd/access/connect-data-sources.html)\n",
    "and create an external configuration from type `GenericWebHDFS` and put the connection name in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_name = 'WebHDFS'\n",
    "hdfs_credentials=icpd_util.get_connection(connection_name, conn_class='external')\n",
    "print(hdfs_credentials) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"create\"> </a> 2. Create the Topology\n",
    "All Streams applications start with a Topology object, so start by creating one topology (topo). <br/>\n",
    "And import the python wrapper for streamsx.hdfs toolkit (hdfs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from streamsx.topology.topology import *\n",
    "from streamsx.topology.context import *\n",
    "from streamsx.topology.schema import StreamSchema\n",
    "import streamsx.hdfs as hdfs\n",
    "import json\n",
    "\n",
    "\n",
    "# create a Topology object\n",
    "topo = Topology(name=\"hdfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"download\"> </a> 2.1. Download the latest version of streamsx.hdfs\n",
    "If you want to work with the latest version of streamsx.hdfs toolkit, it is possible to download the toolkit and add the hdfs toolkit location to your topology.<br/>\n",
    "The list of releases of streamsx.hdfs are in github.<br/>\n",
    "https://github.com/IBMStreams/streamsx.hdfs/releases<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdfs_toolkit_location = hdfs.download_toolkit()\n",
    "#streamsx.spl.toolkit.add_toolkit(topo, hdfs_toolkit_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How to use the streamsx.hdfs package\n",
    "\n",
    "The streamsx.hdfs package is the Python wrapper for the [streamsx.hdfs](https://ibmstreams.github.io/streamsx.hdfs/doc/spldoc/html) toolkit\n",
    "\n",
    "Python package documentation: http://streamsxhdfs.readthedocs.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"sink\"> </a> 2.2. Sink streaming data in a HDFS file\n",
    "\n",
    "Next, we generate a stream of data and write this data in a HDFS file.<br/>\n",
    "But before we start the application, we have to create an HDFS test directory on our hadoop server.<br/>\n",
    "The directory name is in our test: `pytest` . <br/>\n",
    "Login to the hadoop server and start the haddop fs shell tool and create a test directory. <br/>\n",
    "For example:\n",
    "```\n",
    "hadoop fs -mkdir /user/hdfs/pytest \n",
    "```  \n",
    "\n",
    "or for IAE:\n",
    "\n",
    "```\n",
    "hadoop fs -mkdir /user/clsadmin/pytest \n",
    "```  \n",
    "In the first step we write one line in a HDFS file. <br/>\n",
    "The function `fileSinkInputStream` creates a stream that contains one line. <br/> \n",
    "The function `hdfs.write` is the python wrapper of operator `HDFS2FileSink` .<br/>\n",
    "It gets the data streams from `fileSinkInputStream` as input and inserts created lines into a HDFS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'pytest'\n",
    "\n",
    "# creates an input stream and a simple file name\n",
    "fileSinkInputStream = topo.source(['This line will be written into a HDFS file.']).as_string()\n",
    "fileSinkResults = hdfs.write(fileSinkInputStream, credentials=hdfs_credentials, file=dir_name + '/sample_1.txt')\n",
    "fileSinkResults.print(name='printFileSinkResults') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"dynamic_file_sink\"> </a> 2.3. Sink streaming data with dynamic file names into HDFS\n",
    "In the next step we write some lines in several files (dynamic file names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dynamic file names\n",
    "# generates some data with 2 strings schema (Line, FileName)\n",
    "def generate_data():\n",
    "    for counter in range(0, 500):\n",
    "        #yield a random id, name and age\n",
    "        yield  {\"Line\" : \"This is tuple number \" + str(counter), \"FileName\": dir_name + '/sample_' + str(int(counter/50)) + \"_%TIME.txt\"}\n",
    "        time.sleep(0.02)\n",
    "\n",
    "# convert it to SPL schema for the HDFS2FileSink oper.\n",
    "tuple_schema = StreamSchema(\"tuple<rstring Line, rstring FileName>\")\n",
    "\n",
    "# Generates data for a stream created by generate_data of two string attributes.\n",
    "genData = topo.source(generate_data, name=\"GeneratedData\").map(lambda tpl: (tpl[\"Line\"], tpl[\"FileName\"]), schema=tuple_schema)\n",
    "\n",
    "# prints the output streams of genData\n",
    "genData.print(name='printGenData')\n",
    "\n",
    "# The 'fileAttributeName' points to an attribute containing the filename. The operator will close a file when value of this attribute changes.\n",
    "dynamicFileSinkResults = hdfs.write(genData, credentials=hdfs_credentials, file=None, fileAttributeName='FileName')\n",
    "dynamicFileSinkResults.print(name='printdynamicFileSinkResults') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"scan\"> </a> 2.4. Scan all lines from the HDFS file\n",
    "In the next step we scan all files from a HDFS directory.<br/>\n",
    "The function `hdfs.scan` is the python wrapper of operator `HDFS2DiectoryScan` and scans a Hadoop Distributed File System directory for new or modified files and returns the file names.<br/>\n",
    "The parameter `init_delay` specifies the time to wait in seconds before the operator scans the files. <br/>\n",
    "The parameter `directory` specifies the name of the directory to be scanned. <br/>\n",
    "The parameter `pattern`  limits the file names that are listed to the names that match the specified regular expression. <br/>\n",
    "The parameter `strictMode` determines whether the operator reports an error if the directory to be scanned does not exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scans an HDFS directory and return all txt file names that begins with sample (HDFS2DirectoryScan)\n",
    "scannedFileNames = hdfs.scan(topo, credentials=hdfs_credentials, directory=dir_name, pattern='sample.*txt', init_delay=10)\n",
    "\n",
    "# prints scanned file names.\n",
    "scannedFileNames.print(name='printScannedFileNames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"get\"> </a> 2.5. Read lines from the HDFS file\n",
    "Now we read lines from files. <br/>\n",
    "The function `hdfs.read` is the python wrapper of operator `HDFS2FielSource` . <br/>\n",
    "It gets scanned file names from `scan` as input stream, opens the files to read and returns the lines of files. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads lines from a HDFS file (HDFS2FileSource)\n",
    "readLines = hdfs.read(scannedFileNames, credentials=hdfs_credentials)\n",
    "\n",
    "# prints lines \n",
    "readLines.print(name='printReadLines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"create_view\"> </a> 2.6. Create view to show lines from HDFS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view to retrieve lines from a file\n",
    "readView = readLines.view(name=\"readLines\", description=\"shows the lines file HDFS files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"submit\"> </a> 3. Submit the application\n",
    "\n",
    "A running Streams application is called a *job*. This next cell submits the application for execution and prints the resulting job id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.topology import context\n",
    "\n",
    "# Disable SSL certificate verification if necessary\n",
    "cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "# submit the topology 'topo'\n",
    "submission_result = context.submit (\"DISTRIBUTED\", topo, config = cfg)\n",
    "\n",
    "# The submission_result object contains information about the running application, or job\n",
    "if submission_result.job:\n",
    "    streams_job = submission_result.job\n",
    "    print (\"JobId: \", streams_job.id , \"\\nJob name: \", streams_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"view\"> </a> 4. View lines from files\n",
    "Now that the job is started, use the View object you have already created to start retrieving lines from HDFS files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the view and display the lines from HDFS files\n",
    "queue = readView.start_data_fetch()\n",
    "try:\n",
    "    for val in range(20):\n",
    "        print(queue.get(timeout=60))    \n",
    "finally:\n",
    "    readView.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"status\"> </a> 5. See job status\n",
    "\n",
    "You can view job status and logs by going to `My Instances > Jobs`. Find your job based on the id printed above. Retrieve job logs using the `Download logs` action from the job's context menu.\n",
    "\n",
    "To view other information about the job such as detailed metrics, access the graph. Go to **My Instances** > **Jobs**. Select \"View graph\" action for the running job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"cancel\"></a> 6. Cancel the job\n",
    "\n",
    "The Streams job is running in the Streams service. You can cancel it within the notebook or delete it from **My Instances** > **Jobs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel the job directly using the Job object\n",
    "streams_job.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this sample we created an HDFS application which connects to the Hadoop server, inserted some lines in HDFS files and scans a HDFS directory and reads the lines from HDFS files.\n",
    "\n",
    "After submitting the application to the Streams service, we checked the application logs to see the progress. <br/>\n",
    "\n",
    "It is also possible to check the contents of the test directory on the HDFS server with the following `hadoop fs` command. <br/>\n",
    "\n",
    "In this example the hdfsUser is `clsadmin`\n",
    "\n",
    "```\n",
    "hadoop fs -ls /user/clsadmin/pytest\n",
    "```\n",
    "You can delete the test files with this command.   \n",
    "```\n",
    "hadoop fs -rm /user/clsadmin/pytest/*.*\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
