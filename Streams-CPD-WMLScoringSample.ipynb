{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Use WML online scoring on CPD to predict Iris species on streaming data</b></th>\n",
    "  <tr style=\"border: none\">\n",
    "       <th style=\"border: none\"><img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/04/how-it-works.jpg\" width=\"600\" alt=\"Icon\"> </th>\n",
    "   </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the [WML sample notebook](https://github.com/pmservice/wml-sample-notebooks/blob/master/Use%20PMML%20to%20predict%20Iris%20species.ipynb) which shows storing, deploying and online scoring  of the Iris PMML model.\n",
    "It is extended by a section which is using the deployed model to determine the parameters for best performance when handling streaming data.\n",
    "A further section creates a Streaming Analytics application which is using the deployment to score online streaming data.\n",
    "\n",
    "\n",
    "Some familiarity with python is helpful. This notebook uses Python 3.\n",
    "\n",
    "You will use the **Iris** data set to predict the species of an iris flower. This data set contains measurements of the iris perianth flower. \n",
    "\n",
    "## Learning goals\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "-  Work with the Watson Machine Learning (WML) instance\n",
    "  -  Store a PMML model in WML repository\n",
    "  -  Deploy the stored model as Online Deployment \n",
    "  -  Score single data with the deployed model to test the deployment\n",
    "-  Test the deployed model and its performance in the notebook with the streamsx.wml Python package\n",
    "-  Create a Streaming Application with streamsx.wml Python package and deploy it\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1.\t[Work with the WML service instance](#instance) \n",
    "  1.  [Store the model](#store)\n",
    "  2.  [Create a deployment](#deploy)\n",
    "  3.  [Single score test](#singlescore)\n",
    "  \n",
    "2.  [Test the deployment and its performance for streaming data](#test)\n",
    "\n",
    "3.  [Create and run a Streaming Analytics application using this WML online scoring deployment](#application)\n",
    "\n",
    "4.\t[Summary and next steps](#summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instance\"></a>\n",
    "# 1. Working with the WML service instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will learn which information is necessary to connect to WML in CPD and which steps are needed to use the WML API client.\n",
    "\n",
    "You will create and use some WML artifacts (space, model, deployment) which need to have own name. The notebbok defines these names in next cells, they will be used in the code of the notebook later. If you want to use your own name you can change them there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_NAME = \"STREAMS_WML_SAMPLE_SPACE\"\n",
    "MODEL_NAME = \"STREAMS_WML_SAMPLE_MODEL\"\n",
    "DEPLOYMENT_NAME = \"STREAMS_WML_SAMPLE_DEPLOYMENT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authenticate to the WML service on CPD**\n",
    "\n",
    "Running in a Jupyter notebook in CPD you can use the token from the Jupyter environment. This token identifies you in CPD.\n",
    "Below you find the WML credential informations needed to connect to WML in CPD.\n",
    "The WML service URL is addressed by the CPD internal URL (same for all CPD instances), so you don't take care for this.\n",
    "The instance id is a fix value and the version also. Even though this notebook is for CPD 3.0.0 it is using version string \"2.5.0\" for client creation as its using no version \"3.0.0\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "userToken = os.environ[\"USER_ACCESS_TOKEN\"]\n",
    "\n",
    "wml_credentials = {\n",
    "                   \"url\": \"https://internal-nginx-svc:12443\", # access the main CP4D proxy/the CP4D cluster root from internal side\n",
    "                   \"token\": userToken,\n",
    "                   \"instance_id\": \"wml_local\",\n",
    "                   \"version\" : \"2.5.0\"\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to WML in CPD using the WML API client**\n",
    "\n",
    "Create a WML client by using your credentials.\n",
    "\n",
    "To work with the client you need also to set a **SPACE** your client should use.\n",
    "A **SPACE** is where you can train, store and deploy your models. \n",
    "You can have different **SPACES** depending on your needs (e.g. one for TEST and one for PRODUCTION).\n",
    "\n",
    "This notebook creates the space for you if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "wml_client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "\n",
    "spaces = wml_client.spaces.get_details()['resources']\n",
    "space_guid = None\n",
    "for space in spaces:\n",
    "    if space['entity']['name'] == SPACE_NAME:\n",
    "        space_guid = space[\"metadata\"][\"guid\"]\n",
    "if space_guid is None:\n",
    "    space_guid = wml_client.spaces.store(meta_props={wml_client.spaces.ConfigurationMetaNames.NAME: SPACE_NAME})[\"metadata\"][\"guid\"]\n",
    "\n",
    "wml_client.set.default_space(space_guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about the stored models for your WML instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List information about the stored models.\n",
    "wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of the deployed models for your WML service instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List the deployed models.\n",
    "wml_client.deployments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete existing model and deployment\n",
    "\n",
    "As WML doesn't prevent creation of models or deployments with same name the notebook deletes both if they exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_details = wml_client.deployments.get_details()\n",
    "for deployment in deployment_details['resources']:\n",
    "    deployment_id = deployment['metadata']['guid']\n",
    "    model_id = deployment['entity']['asset']['href'].split('/')[3].split('?')[0]\n",
    "    if deployment['entity']['name'] == DEPLOYMENT_NAME:\n",
    "        print('Deleting deployment id', deployment_id)\n",
    "        wml_client.deployments.delete(deployment_id)\n",
    "        print('Deleting model id', model_id)\n",
    "        wml_client.repository.delete(model_id)\n",
    "wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Store the model<a id=\"store\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action**: Use `wget` to download the sample PMML model, `iris_chaid.xml` from the Git project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample PMML model, iris_chaid.xml, from Git.\n",
    "import wget, os\n",
    "\n",
    "sample_dir = 'pmml_sample_model'\n",
    "if not os.path.isdir(sample_dir):\n",
    "    os.mkdir(sample_dir)\n",
    "    \n",
    "filename=os.path.join(sample_dir, 'iris_chaid.xml')\n",
    "if not os.path.isfile(filename):\n",
    "    filename = wget.download('https://github.com/pmservice/wml-sample-models/raw/master/pmml/iris-species/model/iris_chaid.xml', out=sample_dir)\n",
    "    \n",
    "# Save the downloaded file to the WML repository.\n",
    "props_pmml = {wml_client.repository.ModelMetaNames.NAME: MODEL_NAME,\n",
    "              \"runtime\": \"pmml_4.2\", \n",
    "              \"type\": \"pmml_4.2\"}\n",
    "\n",
    "model_details = wml_client.repository.store_model(filename, props_pmml)\n",
    "\n",
    "model_guid = wml_client.repository.get_model_uid(model_details)\n",
    "\n",
    "wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** From the list of downloaded files, you can see that model is successfully stored in the WML service repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Create the online deployment<a id=\"deploy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the online deployment, *Iris species prediction*, for the stored model, then list all the online deployments for the model.\n",
    "\n",
    "**Hint**\n",
    "\n",
    "You give models as well as deployments a name. But the name is not checked if it already exists. The name not not the unique identifier, this is only the **guid** of a model and a deployment.\n",
    "If you take care with your naming the name may be an identifier for you but you need for all API functions the **guid**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deployment = wml_client.deployments.create(model_guid,  meta_props={'name':DEPLOYMENT_NAME,'online':{}})\n",
    "deployment_guid = model_deployment[\"metadata\"][\"guid\"]\n",
    "\n",
    "wml_client.deployments.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** From the list of deployed models, you can see that model was  successfully created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Score data<a id=\"singlescore\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a sample record using the WML Python API client to check that the online scoring endpoint is available. Use here the **guid** of the just created deployment.\n",
    "\n",
    "The data you will send for scoring defines the fields and the field values.\n",
    "You get back the prediction as well as a set of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score data and predict species of flower.\n",
    "scoring_data = [{'fields': ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width'], 'values': [[5.1, 3.5, 1.4, 0.2]]}]\n",
    "\n",
    "predictions = wml_client.deployments.score(deployment_guid,meta_props={'input_data':scoring_data})\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the prediction, this is the Iris Setosa flower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>\n",
    "# 2. Test the performance of your online scoring deployment\n",
    "\n",
    "The streamsx.wml Python package allows to use each WML online deployment to score streaming data. \n",
    "WML online scoring is based on a REST API. As such it has a poor performance when each input data is scored in single REST request.\n",
    "But the performance of WML online scoring can be increased by \n",
    "-  sending multiple input data in one request (mini batch paradigm)\n",
    "-  having multiple request sender \n",
    "-  deploying multple node\n",
    "\n",
    "The streamsx.wml Python package is using all these possibilities to increase the WML REST performance.\n",
    "\n",
    "The below performance test code can be used to determine the influence of different settings for number of input data in a request and request senders (Python threads).\n",
    "If a suffiecient performance is found the appropriate settings can be taken as parameters for the Streaming Analytics application which will score your real time data.\n",
    "\n",
    "There are some points to have in mind:\n",
    "-  having a failure in a request with multiple input data will lead to fail the complete request \n",
    "-  so, the higher the number of input data in one request the higher the number to be processed afterwards one by one in case of failure, which takes longer time\n",
    "-  so, input data quality is of high importance to use this mini batch paradigm\n",
    "-  increasing thread number will work only to certain level, delays caused by Python thread handling will get important\n",
    "-  actual input data sequence is kept, which may lead to postpone threads which got results earlier than the the ones with with earlier data (especially when using multiple deployment nodes)\n",
    "-  In the used version of streamsx.wml the interface doesn't provide direct setting the amount of input data in one request but it is computed from a expected_load number and thread and node count ( __bundle_size = expected_load/(threads_per_node * node_count)__ ). But experience showed that this is not best way. Later versions will change this.\n",
    "\n",
    "The below code can be taken for any WML online deployment when __field_mapping_dict__ (which assigns streaming data attributes to the mining fields the model expects) and __class TestSource__ are changed according to the model which is deployed.\n",
    "\n",
    "__space_guid__ and __deployment_guid__ are set by the code cells just ran before.\n",
    "\n",
    "The performance which will be determined here in the notebook will be higher as the one you will get when you use same parameters for the Streaming Application later. There is addidional processing time consumed for sending and receiving data in the Streams runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamsx.wml\n",
    "streamsx.wml.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.wml.bundleresthandler.wmlbundlecontroller import WmlBundleController\n",
    "import streamsx.wml.utils as wml_utils\n",
    "import threading\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "# Parameters\n",
    "#\n",
    "# Collection all the parameters needed for using WML online scoring \n",
    "# resp. the underlying Rest Bundlehandler classes \n",
    "# \n",
    "# deployment_guid and space_guid are set by the code ran before.\n",
    "# If you want to run this code w/o running the steps before you\n",
    "# need to set the appropriate guids below and uncomment the 2 lines.\n",
    "###################################################################################\n",
    "#deployment_guid = <your_guid>\n",
    "#space_guid = <your_guid>\n",
    "\n",
    "field_mapping_dict =[{\"model_field\":\"Sepal.Length\",\n",
    "                      \"is_mandatory\":True,\n",
    "                      \"tuple_field\":\"sepal_length\"},\n",
    "                     {\"model_field\":\"Sepal.Width\",\n",
    "                      \"is_mandatory\":True,\n",
    "                      \"tuple_field\":\"sepal_width\"},\n",
    "                     {\"model_field\":\"Petal.Length\",\n",
    "                      \"is_mandatory\":True,\n",
    "                      \"tuple_field\":\"petal_length\"},\n",
    "                     {\"model_field\":\"Petal.Width\",\n",
    "                      \"is_mandatory\":True,\n",
    "                      \"tuple_field\":\"petal_width\"}]\n",
    "\n",
    "\n",
    "# streamsx.wml functions support field mapping as JSON \n",
    "field_mapping = json.dumps(field_mapping_dict)\n",
    "# streamsx.wml functions support WML credentials as JSON \n",
    "wml_credentials = json.dumps(wml_utils.get_wml_credentials(version = '2.5.0'))  #token,url,instance_id,version\n",
    "\n",
    "######################################################\n",
    "# Callable class used by the REST handler threads.\n",
    "# It's being called with the results of processed\n",
    "# records bundle.\n",
    "######################################################\n",
    "class OutputClass:\n",
    "    def __init__(self, interval):\n",
    "        self._timer_interval = interval\n",
    "        self._record_counter = 0\n",
    "        self._result_1_counter = 0\n",
    "        self._result_2_counter = 0\n",
    "        self._time_a = time.time()\n",
    "        self._last_count = 0\n",
    "        self._timer = threading.Timer(interval,self._timer_action)\n",
    "    \n",
    "    def _timer_action(self):\n",
    "        load = (self._record_counter - self._last_count) / (time.time() - self._time_a)\n",
    "        print('Load(1/s): ', load, '   Records: ', self._record_counter, '   Records on output 1: ',self._result_1_counter,'   Failed records on output 2: ',self._result_2_counter)\n",
    "         \n",
    "    def __call__(self, results):\n",
    "        if not self._timer.is_alive():\n",
    "            self._timer = threading.Timer(self._timer_interval,self._timer_action)\n",
    "            self._time_a = time.time()\n",
    "            self._last_count = self._record_counter\n",
    "            self._timer.start()\n",
    "        for index,result_list in enumerate(results):\n",
    "            # first returned list are the successful scored record \n",
    "            #(and ... depending on parameter single_output ... the failed)\n",
    "            if index == 0:\n",
    "                for list_element in result_list:\n",
    "                    self._record_counter +=1\n",
    "                    self._result_1_counter += 1\n",
    "            # second returned list are the failed records \n",
    "            # if 'single_output = False'\n",
    "            elif index == 1:\n",
    "                for list_element in result_list:\n",
    "                    self._record_counter +=1\n",
    "                    self._result_2_counter += 1\n",
    "            else:\n",
    "                print(\"Internal error: More result lists generated than supported. \")\n",
    "        \n",
    "\n",
    "\n",
    "            \n",
    "###################################################################################\n",
    "# Test source\n",
    "#\n",
    "# Class with callable generating data as needed for the model which should be used.\n",
    "# In streaming application the connect stream is the source.\n",
    "###################################################################################\n",
    "class TestSource:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    def __call__(self):\n",
    "            # run this indefinitely so that there will always be data for the view\n",
    "        counter = 0\n",
    "        while True:\n",
    "            counter += 1\n",
    "            record = {\"petal_length\":1.4,\n",
    "                       \"petal_width\":0.2,\n",
    "                       \"sepal_length\":5.1,\n",
    "                       \"sepal_width\":3.5,\n",
    "                       \"number\" : counter}\n",
    "            # generate errors\n",
    "            #if counter % 20 == 0:\n",
    "            #    record.pop(\"petal_length\",None)\n",
    "            #    record.pop(\"sepal_length\",None)\n",
    "\n",
    "            #yield everytime same values, doesn't matter for test\n",
    "            yield record\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "# Test class\n",
    "#\n",
    "# The test application which is using the streamsx.wml WmlBundlerController, \n",
    "# same as the one used in function for Streaming Analytics applications provided by streamsx.wml\n",
    "###################################################################################\n",
    "class PerformanceTest:\n",
    "    def __init__(self,source, **params):\n",
    "        self._params = params\n",
    "        self._source_generator = source()\n",
    "\n",
    "    def _generate_data(self):\n",
    "        while self._run:\n",
    "            #for data in TestSource():\n",
    "            self._testcontroller.process_data(next(self._source_generator))\n",
    "\n",
    "    def start(self):\n",
    "        self._testcontroller = WmlBundleController(**self._params)\n",
    "        self._testcontroller.prepare()\n",
    "        self._testcontroller.run()\n",
    "        self._run = True\n",
    "        self._source_thread = threading.Thread(target=self._generate_data)\n",
    "        self._source_thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self._run = False\n",
    "        self._source_thread.join()\n",
    "        self._testcontroller.stop()\n",
    "        self._testcontroller.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test instance with your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Dictionary of all parameters\n",
    "#\n",
    "# as used also in Streaming application\n",
    "# except the 'output_function' parameter which is \n",
    "# added by the streaming operator as the output is\n",
    "# special for any implementation using the \n",
    "# WmlBundleController\n",
    "####################################################            \n",
    "params= {\n",
    "    'deployment_guid':deployment_guid,\n",
    "    'wml_credentials':wml_credentials,\n",
    "    'space_guid':space_guid,\n",
    "    'expected_load':  500,\n",
    "    'queue_size': 1000, \n",
    "    'threads_per_node': 5,\n",
    "    'node_count':1,\n",
    "    'field_mapping':field_mapping, \n",
    "    'output_function':OutputClass(interval=5.0 ),\n",
    "    'single_output': False}\n",
    "\n",
    "test = PerformanceTest(TestSource(), **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the test\n",
    "\n",
    "It will print every 5 secods a metrics as following:\n",
    "\n",
    "`Load(1/s):  10194.683301388393    Records:  51000    Records on output 1:  51000    Failed records on output 2:  0`\n",
    "\n",
    "It shows the actual determined load for the 5s intervall the records/input data processed in this time and the records/scoring results which were successful (output 1) and failed (output 2). Failed will be seen only when you set your __class TestSource__ to generate some error input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the test\n",
    "\n",
    "__!!!Please stop the test before you change the parameters and start again!!!__\n",
    "\n",
    "There are threads running which need to be stopped!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"application\"></a>\n",
    "# 3. Create and run a Streamiing Analytics application using this WML online scoring deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"setup\"></a>\n",
    "\n",
    "## 3.1. Setup\n",
    "\n",
    "  \n",
    "  \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>What do we need to set up for our sample application?</b>  \n",
    "<ul>    \n",
    "<li>A connection to a running <b>IBM Streams instance</b> <br> \n",
    "   This notebook describes using a Streams instance in IBM Cloud Pak for Data.</li>\n",
    "<li><b>streamsx</b> - the Python package providing the IBM Streams standard/basic function set</li>\n",
    "<li><b>streamsx.wml</b> - the Python package providing the IBM Streams WML online scoring functionality</li>\n",
    "<li><b>Data set</b> with sample Iris data</li>\n",
    "<li><b>WML deployment</b> of the PMML Iris species prediction model (done above) </li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"setupStreams\"></a>\n",
    "\n",
    "### 3.1.1 Add credentials for the IBM Streams service\n",
    "\n",
    "In order to submit a Streams application you need to provide the name of the Streams instance.\n",
    "\n",
    "1. From the navigation menu, click **My instances**.\n",
    "2. Click the **Provisioned Instances** tab.\n",
    "3. Update the value of `streams_instance_name` in the cell below according to your Streams instance name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icpd_core import icpd_util\n",
    "streams_instance_name = \"streams\" ## Change this to Streams instance\n",
    "cfg=icpd_util.get_service_instance_details(name=streams_instance_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setupPackages\"></a>\n",
    "\n",
    "### 3.1.2 Import  the `streamsx` and `streamsx.wml` package and verify the package version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import streamsx.topology.context\n",
    "import streamsx.wml\n",
    "print(\"INFO: streamsx package version: \" + streamsx.topology.context.__version__)\n",
    "print(\"INFO: streamsx.wml package version: \" + streamsx.wml.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setupDataModel\"></a>\n",
    "\n",
    "### 3.1.3 Add test data your project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data from Git.\n",
    "import wget, os\n",
    "\n",
    "sample_dir = 'pmml_sample_model'\n",
    "if not os.path.isdir(sample_dir):\n",
    "    os.mkdir(sample_dir)\n",
    "    \n",
    "filename_data=os.path.join(sample_dir, 'iris.csv')\n",
    "if not os.path.isfile(filename_data):\n",
    "    filename_data = wget.download('https://ibm.box.com/shared/static/nnxx7ozfvpdkjv17x4katwu385cm6k5d.csv', out=sample_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create\"></a>\n",
    "\n",
    "## 3.2. Create the application\n",
    "\n",
    "\n",
    "All Streams applications start with  a `Topology` object, so we start by creating one.<br>Additionally we add our data set to the topology as a dependency, which means that is bundled within the built application and so available at the environment where the application will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.topology.topology import Topology\n",
    "import os\n",
    "\n",
    "topo = Topology(name=\"WMLScoring\")\n",
    "\n",
    "# add files to be contained in the archive which is deployed to the node running the application\n",
    "# in this sample we need the Iris `dataset` with sample data to be present at the worker node\n",
    "topo.add_file_dependency(filename_data, 'etc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"defineInput\"></a>\n",
    "\n",
    "### 3.2.1 Define Data Input \n",
    "\n",
    "Let's take a look at the sample data, a set of measured data records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = filename_data\n",
    "!head $sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this CSV file as data source for this sample. To simulate an endless stream of data we will read the file repeatedly and submit each line in the file as a tuple. The class below reads the data set and produces a stream of Python `dict` objects. The produced `Stream` is a stream of Python `dict`.\n",
    "\n",
    "`Stream data : {\"sepal_length\" : <sepal_length_input>,\n",
    "               \"sepal_width\"  : <sepal_width_input>, \n",
    "               \"petal_length\" : <petal_length_input>,\n",
    "               \"petal_width\"  : <petal_width_input>,\n",
    "               \"referenceSpecies\" : <species_target>,\n",
    "               \"Prediction\" : \"\"}`\n",
    "\n",
    "\"Prediction\" will be the field which contains the result from scoriing. Depending on model and model framework the \"Prediction\" may be single value or a dictionary of multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from streamsx.ec import get_application_directory\n",
    "import time \n",
    "\n",
    "# a class which can be used in topology as a source\n",
    "class CSVFileReader:\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "    def __call__(self):\n",
    "        # Convert each row in the file to a dict\n",
    "        drug_file =  os.path.join(get_application_directory(), \"etc\", self.file_name)\n",
    "        col_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"referenceSpecies\"]\n",
    "       \n",
    "        # run this indefinitely so that there will always be data for the view\n",
    "        while True:\n",
    "            with open(drug_file) as handle:\n",
    "                reader = csv.DictReader(handle, delimiter=',',\n",
    "                                                fieldnames=col_names)\n",
    "                #Use this to skip the header line if your file has one\n",
    "                next(reader)\n",
    "                #yield the lines in the file one at a time\n",
    "                #throttle\n",
    "                throttle_count = 0\n",
    "                for row in reader:\n",
    "                    throttle_count += 1\n",
    "                    if throttle_count % 4 is 0:\n",
    "                        throttle_count = 0\n",
    "                        time.sleep(0.0001)\n",
    "                    yield dict(sepal_length= float(row[\"sepal_length\"]),\n",
    "                                     sepal_width = float(row[\"sepal_width\"]), \n",
    "                                    petal_length= float(row[\"petal_length\"]),\n",
    "                                    petal_width=float(row[\"petal_width\"]) ,\n",
    "                                    referenceSpecies=row[\"referenceSpecies\"],\n",
    "                                    predictedSpecies=\"\")\n",
    "\n",
    "# let the csv file reader the source/edge node in our topology, producing the 'records' stream                    \n",
    "records = topo.source(CSVFileReader(\"iris_initial.csv\"))\n",
    "# Let the source run in separate Python interpreter instance.\n",
    "# streamsx.wml.wml_online_scoring() is a Python function and for bestt performance it will need all available ressources of an own Python interpreter instance.\n",
    "records.isolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"analyzeData\"></a>\n",
    "### 3.2.2 Score data and run the application\n",
    "\n",
    "\n",
    "Now that we have created a stream of Iris data records, we need to define the analytics we want to perform on our data. Our WML deployment (PMML model) is predicting the Iris species from the measured data.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<p>These are the core steps you need for using WML model scoring in your application:</p>\n",
    "<ul>\n",
    "    <li><b>Data preparation</b><br>Make sure your input stream meets your deployments (models) expected input features\n",
    "    </li>\n",
    "    <li><b>Score using <code>streamsx.wml.wml_online_scoreing()</code>, specifying: </b><br>\n",
    "        - the input stream which should be processed <b>mandatory</b><br>\n",
    "        - the deployment_guid of the deployment we want to use <b>mandatory</b><br>\n",
    "        - the field_mapping from input stream fields to model input features <b>mandatory</b><br>\n",
    "        - the credentials to get access to the deployment <b>mandatory</b><br>\n",
    "        - the space_guid of the deployment <b>mandatory</b><br>\n",
    "        - the queue_size <br>\n",
    "        - the parameters to calculate the bundle size  <b><code>bundle_size = expected_load/(threads_per_node * node_count)</code></b>\n",
    "        <ul><li>expected_load</li>\n",
    "            <li>threads_per_node</li>\n",
    "            <li>node_count</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<p>Next versions will provide <b>bundle_size</b> parameter directly, as it showed that this formula was not best choice. But up to there you will need to set the expected_load so that you together with node_count and threads_per_node get a bundle_size of your choice.</p>\n",
    "<p>In the code below we use values which were seen to give a good performance. It is assumed that there is one deployment node created. You may later play around with the values to see if they match the expectations you have from the performance test before</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.topology import context\n",
    "\n",
    "#####################################################################################\n",
    "# Define the parameter for the scoring function\n",
    "#####################################################################################\n",
    "field_mapping_dict =[{\"model_field\":\"Sepal.Length\",\n",
    "                      \"is_mandatory\":True,\n",
    "                      \"tuple_field\":\"sepal_length\"},\n",
    "                     {\"model_field\":\"Sepal.Width\",\n",
    "                      \"is_mandatory\":True,\n",
    "                      \"tuple_field\":\"sepal_width\"},\n",
    "                     {\"model_field\":\"Petal.Length\",\n",
    "                      \"is_mandatory\":True,\n",
    "                      \"tuple_field\":\"petal_length\"},\n",
    "                     {\"model_field\":\"Petal.Width\",\n",
    "                      \"is_mandatory\":True,\n",
    "                      \"tuple_field\":\"petal_width\"}]\n",
    "\n",
    "# streamsx.wml.wml_online_scoring() support field mapping as JSON or dict\n",
    "field_mapping = json.dumps(field_mapping_dict)\n",
    "\n",
    "# streamsx.wml.wml_online_scoring() functions support WML credentials as JSON or dict\n",
    "wml_credentials = json.dumps(wml_utils.get_wml_credentials(version = '2.5.0'))  #token,url,instance_id,version\n",
    "\n",
    "# function call parameter\n",
    "params= {\n",
    "    'deployment_guid':deployment_guid, #the one from the cells run above\n",
    "    'field_mapping':field_mapping, \n",
    "    'credentials':wml_credentials,\n",
    "    'space_guid':space_guid,           #the one from the cells run above\n",
    "    'expected_load':  2000,\n",
    "    'queue_size': 10000, \n",
    "    'threads_per_node': 2,\n",
    "    'node_count':1,\n",
    "    'single_output': False}\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "# Add the scoring function to your topology\n",
    "#\n",
    "# 2 result streams are generated: \n",
    "#        1st for successful scorings, \n",
    "#        2nd for failed scorings or invalid input\n",
    "####################################################################################\n",
    "scorings,invalids = streamsx.wml.wml_online_scoring(records, \n",
    "                                                  **params)\n",
    "\n",
    "\n",
    "scorings.isolate()\n",
    "scorings_mapped = scorings.map(lambda x: x)\n",
    "\n",
    "# publish results as JSON\n",
    "#scorings.publish(topic=\"ScoredRecords\",schema=json,name=\"PublishScores\")\n",
    "#score_view = scorings.view(name=\"ScoredRecords\", description=\"Sample of scored records\")\n",
    "#scorings_mapped.publish(topic=\"ScoredRecords\",schema=json,name=\"PublishScores\")\n",
    "#score_view = scorings_mapped.view(name=\"ScoredRecords\", description=\"Sample of scored records\")\n",
    "\n",
    "\n",
    "# Disable SSL certificate verification if necessary\n",
    "cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "# Build and submit your topology\n",
    "#####################################################################################\n",
    "submission_result = context.submit('DISTRIBUTED', \n",
    "                                   topo, \n",
    "                                   cfg)\n",
    "\n",
    "print(submission_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Use the View to access data from the job\n",
    "\n",
    "Now that the job is started, use the View object you have already created to start retrieving successfully scored data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the view and display the selected data\n",
    "queue = score_view.start_data_fetch()\n",
    "try:\n",
    "    for val in range(20):\n",
    "        print(queue.get(timeout=60))    \n",
    "finally:\n",
    "    score_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.4 See job status\n",
    "\n",
    "You can view job status and logs by going to My Instances > Jobs. Find your job based on the id printed above. Retrieve job logs using the \"Download logs\" action from the job's context menu.\n",
    "\n",
    "To view other information about the job such as detailed metrics, access the graph. Go to My Instances > Jobs. Select \"View graph\" action for the running job\n",
    "\n",
    "\n",
    "## 3.5 Cancel the application\n",
    "The Streams job is running in the Streams service. You can cancel it within the notebook or delete it from My Instances > Jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel the job in the IBM Streams service\n",
    "submission_result.cancel_job_button()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "# 4. Summary and next steps     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You successfully completed this notebook! \n",
    "<p>You learned how to use Watson Machine Learning for PMML model storage, deployment and scoring.</p>\n",
    "<p>You have further used the deployment to run a performance test in this notebook to see how many scorings per second can be reached with the WML provided <b>mini-batch</b> approach.</p>\n",
    "<p>Last you have created a Streaming Analytics application which is using the WML deployment for online scoring of its streaming data. You saw that you can use same parameters set in performance test.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "Dua, D. and Karra Taniskidou, E. (2017). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
