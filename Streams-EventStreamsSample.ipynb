{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IBM Streams Event Streams sample application\n",
    "\n",
    "This sample demonstrates how to create a Streams Python application that ingests data into the [IBM Event Streams](https://cloud.ibm.com/catalog?search=Event%20Streams) service, and consumes the data from Event Streams. The IBM Event Streams service is a fully managed Kafka Service within the IBM cloud.\n",
    "\n",
    "In this notebook, you'll see examples of how to :\n",
    " 1. [Setup your data connections](#setup)\n",
    " 2. [Create the application](#create)\n",
    " 3. [Submit the application](#launch)\n",
    " 4. [Connect to the running application to view data](#view)\n",
    " 5. [Stop the application](#cancel)\n",
    "\n",
    "# Overview\n",
    "\n",
    "**About the sample**\n",
    "\n",
    "This application creates artificial sensor data and writes them into a topic in the IBM Event Streams instance, subscribes to the same topic and filters out the data of one sensor.\n",
    "\n",
    "**How it works**\n",
    "\n",
    "The Python application created in this notebook is submitted to the IBM Streams service for execution. Once the application is running in the service, you can connect to it from the notebook to retrieve the results.\n",
    "\n",
    "<img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/04/how-it-works.jpg\" alt=\"How it works\">\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"setup\"></a>\n",
    "# 1. Setup\n",
    "### 1.1 Add credentials for the IBM Streams service\n",
    "\n",
    "In order to submit a Streams application you need to provide the name of the Streams instance.\n",
    "\n",
    "1. From the navigation menu, click **My instances**.\n",
    "2. Click the **Provisioned Instances** tab.\n",
    "3. Update the value of `streams_instance_name` in the cell below according to your Streams instance name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icpd_core import icpd_util\n",
    "streams_instance_name = \"sample-streams\" ## Change this to Streams instance\n",
    "\n",
    "try:\n",
    "    cfg=icpd_util.get_service_instance_details(name=streams_instance_name, instance_type=\"streams\")\n",
    "except TypeError:\n",
    "    cfg=icpd_util.get_service_instance_details(name=streams_instance_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import  the `streamsx.kafka` package and verify the package version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamsx.kafka as kafka\n",
    "import streamsx.topology.context\n",
    "print(\"INFO: streamsx package version: \" + streamsx.topology.context.__version__)\n",
    "print(\"INFO: streamsx.kafka package version: \" + kafka.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Configure the connection to the IBM Event Streams service\n",
    "\n",
    "To connect with the Event Streams cloud service, we need service credentials, and at least one topic within the service instance.\n",
    "\n",
    "To create the credentials and a topic, do the following steps:\n",
    "\n",
    "1. Create an Event Streams service instance on IBM cloud.\n",
    "\n",
    "   You need to have an IBM account to be able to do this.\n",
    "   \n",
    "   https://cloud.ibm.com/catalog?search=Event%20Streams\n",
    "   <br>\n",
    "   \n",
    "1. Under *Topics*, create one topic. You can use the default values for all settings. The topic name will be used later in the notebook.\n",
    "1. Under *Service credentials*, create new credentials. You can leave all settings at their defaults.\n",
    "1. View the created credentials, and copy them to the clipboard\n",
    "1. Paste the credential into the `Your Event Streams credentials:` prompt in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "eventstreams_credentials_json = getpass.getpass('Your Event Streams credentials:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Kafka connection configuration and store it in an application configuration in the IBM Streams service. This is the safest way to avoid the credentials being exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an application configuration\n",
    "from streamsx.rest import Instance\n",
    "\n",
    "cfg[streamsx.topology.context.ConfigParams.SSL_VERIFY] = False\n",
    "instance = Instance.of_service(cfg)\n",
    "app_config_name = kafka.configure_connection_from_properties(\n",
    "    instance=instance,\n",
    "    name='kafka_event_streams',\n",
    "    properties=kafka.create_connection_properties_for_eventstreams(eventstreams_credentials_json),\n",
    "    description='Kafka connection for event streams')\n",
    "print(\"INFO: Name of your application configuration: \" + app_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Event Streams service, create the *topic* where you want to publish the data. You can use the default settings for partitions and retention hours. Enter the topic name before you run the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"my_topic\"  ## change this to an existing topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"create\"></a>\n",
    "# 2. Create the application\n",
    "\n",
    "This application is going to ingest readings from simulated sensors into a topic in the Event Streams service. Another part of the application subscribes to the topic and filters out one sensor of interest.  \n",
    "\n",
    "All Streams applications start with  a `Topology` object, so start by creating one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.topology.topology import Topology\n",
    "\n",
    "topo = Topology(name=\"EventStreamsSample\", namespace=\"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define sources\n",
    "Your application needs some data to analyze, so the first step is to define a data source that produces the data being processed. \n",
    "\n",
    "Next, use the data source to create a `Stream` object. A `Stream` is a potentially infinite sequence of tuples containing the data to be analyzed.\n",
    "\n",
    "In this example, we use JSON objects, which are Python dicts. Other supported formats include Strings, structured tuples, and more. [See the doc for all supported formats](http://ibmstreams.github.io/streamsx.topology/doc/pythondoc/streamsx.topology.topology.html#stream)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Define a source class\n",
    "\n",
    "Define a *callable* class that will produce the data to be analyzed.\n",
    "\n",
    "This example class produces readings from sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# define a callable source \n",
    "class SensorReadingsSource(object):\n",
    "    def __call__(self):\n",
    "        # This is just an example of using generated data, \n",
    "        # Here you could connect to db\n",
    "        # generate data\n",
    "        # connect to data set\n",
    "        # open file\n",
    "        \n",
    "        while True:\n",
    "            time.sleep(0.005)\n",
    "            sensor_id = random.randint(1,100)\n",
    "            reading = {}\n",
    "            reading[\"sensor_id\"] = \"sensor_\" + str(sensor_id)\n",
    "            reading[\"value\"] =  random.random() * 3000\n",
    "            reading[\"ts\"] = int((datetime.now().timestamp()))\n",
    "            yield reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2  Create the `Stream `\n",
    "\n",
    "Create a `Stream` with `CommonSchema.Json` schema called  `Readings` that will contain the simulated data that `SensorReadingsSource` produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stream from the data using Topology.source\n",
    "readings = topo.source(SensorReadingsSource(), name=\"Readings\").as_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Publish the tuples in the Event Streams service\n",
    "\n",
    "Now publish the data of the `readings` stream to the topic you have configured in the `topic` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.kafka import KafkaProducer\n",
    "readings.for_each(KafkaProducer(config=app_config_name, topic=topic),\n",
    "                  name='EventStrPublish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "By now, you have defined a streaming application that generates simulated data and publishes the data in a topic within an Event Streams service. You could submit the application now, so that any other application could consume the data from the Event Streams service.\n",
    "\n",
    "In the next steps, you extend the `topo` topology by a consumer that consumes and analyzes the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Subscribe to the Event Streams topic and consume the data\n",
    "\n",
    "When you subscribe to the topic, you create a new source, that connects to the Event Streams service. The stream of data shall have the  `Json` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.topology.schema import CommonSchema\n",
    "from streamsx.kafka import KafkaConsumer\n",
    "# create a new Json stream in the topology\n",
    "\n",
    "sensordata = topo.source(KafkaConsumer(config=app_config_name, \n",
    "                                       topic=topic, \n",
    "                                       schema=CommonSchema.Json),\n",
    "                         name='EventStrSubscribe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Analyze the data\n",
    "\n",
    "Use a variety of methods in the `Stream` class to analyze your in-flight data, including applying machine learning models.\n",
    "\n",
    "See the [common operations section](https://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-4/) of the developer guide and the [documentation on the Stream class](https://ibmstreams.github.io/streamsx.topology/doc/pythondoc/streamsx.topology.topology.html#streamsx.topology.topology.Stream) for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Filter data from the  `Stream`  \n",
    "\n",
    "Use `Stream.filter()` to pass through only data that match a certain condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example, pass through only sensor data from sensor with ID \"sensor_3\"\n",
    "\n",
    "sensordata_id3 = sensordata.filter(lambda x: x[\"sensor_id\"] == \"sensor_3\",\n",
    "                                   name=\"SensorsId3\")\n",
    "\n",
    "# you could create another stream of the other sensors:\n",
    "#sensordata_other = sensordata.filter(lambda x: x[\"sensor_id\"] != \"sensor_3\", name=\"OtherSensors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Create a `View` to preview the tuples on the `Stream` \n",
    "\n",
    "\n",
    "A `View` is a connection to a `Stream` that becomes activated when the application is running. We examine the data from within the notebook in section 4, below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor3_view = sensordata_id3.view(name=\"Sensor3\",\n",
    "                                   description=\"Sample of sensor with ID sensor_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Define output\n",
    "\n",
    "The `sensordata_id3` stream is our final result. We will use `Stream.publish()` to make this stream available to other Streams applications. \n",
    "\n",
    "If you want to send the stream to another database or system, you would use a sink function (similar to the source function) and invoke it using `Stream.for_each`.\n",
    "\n",
    "You can also the functions of other Python packages to send the stream to other systems, for example the eventstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# publish results as JSON\n",
    "sensordata_id3.publish(topic=\"SensorData\",\n",
    "                       schema=json,\n",
    "                       name=\"PublishSensors\")\n",
    "\n",
    "# other options include:\n",
    "# invoke another sink function:\n",
    "#sensordata_id3.for_each(func=send_to_db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"launch\"></a>\n",
    "\n",
    "# 3. Submit the application\n",
    "A running Streams application is called a *job*. This next cell submits the application for execution and prints the resulting job id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from streamsx.topology import context\n",
    "\n",
    "# disable SSL certificate verification if necessary\n",
    "cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "# submit the topology 'topo'\n",
    "submission_result = context.submit(\"DISTRIBUTED\", topo, config=cfg)\n",
    "\n",
    "# the submission_result object contains information about the running application, or job\n",
    "if submission_result.job:\n",
    "    streams_job = submission_result.job\n",
    "    print(\"JobId: \", streams_job.id , \"\\nJob name: \", streams_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"view\"></a>\n",
    "\n",
    "# 4. Use a `View` to access data from the job\n",
    "Now that the job is started, use the `View` object you created in step 2.3 to start retrieving data from a `Stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the view and display the data\n",
    "queue = sensor3_view.start_data_fetch()\n",
    "try:\n",
    "    for val in range(10):\n",
    "        print(queue.get(timeout=60))    \n",
    "finally:\n",
    "    sensor3_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Display the results in real time\n",
    "Calling `View.display()` from the notebook displays the results of the view in a table that is updated in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the results for 30 seconds\n",
    "sensor3_view.display(duration=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 See job status \n",
    "\n",
    "You can view job status and logs by going to **My Instances** > **Jobs**. Find your job based on the id printed above.\n",
    "Retrieve job logs using the \"Download logs\" action from the job's context menu.\n",
    "\n",
    "To view other information about the job such as detailed metrics, access the graph. Go to **My Instances** > **Jobs**. Select \"View graph\" action for the running job.\n",
    "\n",
    "\n",
    "<a name=\"cancel\"></a>\n",
    "\n",
    "# 5. Cancel the job\n",
    "\n",
    "The Streams job is running in the Streams service. You can cancel it within the notebook or delete it from **My Instances** > **Jobs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel the job in the IBM Streams service\n",
    "streams_job.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We started with a `Stream` called `readings`, which contained the data that we published in the Event Streams service. Next, we created a new Stream `sensordata` by subscribing to the topic in the Event Streams Service, filtered out one sensor of interest, and `published` the filtered stream for other applications running within our Streams instance to access.\n",
    "\n",
    "After submitting the application to the IBM Streams service, we connected to the `sensor3_view` view to see the data of sensor 3 within the notebook.\n",
    "\n",
    "You may have noticed that the application consists of two independent parts: One part generates the data and publishes the them to the Event Streams cloud service. The other part consumes from Event Streams, filters, and publishes the stream within the IBM Streams instance. These two parts can also be declared by using different topologies, and can be submitted as separate jobs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
